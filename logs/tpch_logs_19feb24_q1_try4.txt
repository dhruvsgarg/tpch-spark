24/02/19 12:35:27 WARN Utils: Your hostname, rivertam resolves to a loopback address: 127.0.1.1; using 130.207.125.81 instead (on interface ens6f0)
24/02/19 12:35:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
24/02/19 12:35:27 INFO SparkContext: Running Spark version 4.0.0-SNAPSHOT
24/02/19 12:35:27 INFO SparkContext: OS info Linux, 5.4.0-152-generic, amd64
24/02/19 12:35:27 INFO SparkContext: Java version 17.0.9-internal
24/02/19 12:35:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/02/19 12:35:27 INFO SparkContext: Application specified no deadline, will be set to Int.MaxValue (string)
24/02/19 12:35:27 INFO SparkContext: Application specified no tpch_qtype, will be set to NONE
24/02/19 12:35:27 INFO ResourceUtils: ==============================================================
24/02/19 12:35:27 INFO ResourceUtils: No custom resources configured for spark.driver.
24/02/19 12:35:27 INFO ResourceUtils: ==============================================================
24/02/19 12:35:27 INFO SparkContext: Submitted application: TPC-H v3.0.0 Spark
24/02/19 12:35:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/02/19 12:35:27 INFO ResourceProfile: Limiting resource is cpu
24/02/19 12:35:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/02/19 12:35:27 INFO SecurityManager: Changing view acls to: dgarg39
24/02/19 12:35:27 INFO SecurityManager: Changing modify acls to: dgarg39
24/02/19 12:35:27 INFO SecurityManager: Changing view acls groups to: 
24/02/19 12:35:27 INFO SecurityManager: Changing modify acls groups to: 
24/02/19 12:35:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dgarg39; groups with view permissions: EMPTY; users with modify permissions: dgarg39; groups with modify permissions: EMPTY; RPC SSL disabled
24/02/19 12:35:28 INFO Utils: Successfully started service 'sparkDriver' on port 42811.
24/02/19 12:35:28 INFO SparkEnv: Registering MapOutputTracker
24/02/19 12:35:28 INFO SecurityManager: Changing view acls to: dgarg39
24/02/19 12:35:28 INFO SecurityManager: Changing modify acls to: dgarg39
24/02/19 12:35:28 INFO SecurityManager: Changing view acls groups to: 
24/02/19 12:35:28 INFO SecurityManager: Changing modify acls groups to: 
24/02/19 12:35:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dgarg39; groups with view permissions: EMPTY; users with modify permissions: dgarg39; groups with modify permissions: EMPTY; RPC SSL disabled
24/02/19 12:35:28 INFO SparkEnv: Registering BlockManagerMaster
24/02/19 12:35:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/02/19 12:35:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/02/19 12:35:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/02/19 12:35:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3edf61b7-ad65-4dfe-8d29-7f3503d117a6
24/02/19 12:35:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/02/19 12:35:28 INFO SparkEnv: Registering OutputCommitCoordinator
24/02/19 12:35:28 INFO log: Logging initialized @1717ms to org.eclipse.jetty.util.log.Slf4jLog
24/02/19 12:35:28 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/02/19 12:35:28 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 17.0.9-internal+0-adhoc..src
24/02/19 12:35:28 INFO Server: Started @1797ms
24/02/19 12:35:28 INFO AbstractConnector: Started ServerConnector@6a4e09fe{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
24/02/19 12:35:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2149594a{/,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO SparkContext: Added JAR file:/home/dgarg39/tpch-spark/target/scala-2.13/spark-tpc-h-queries_2.13-1.0.jar at spark://130.207.125.81:42811/jars/spark-tpc-h-queries_2.13-1.0.jar with timestamp 1708364127566
24/02/19 12:35:28 INFO SparkContext: DG: init TaskScheduler w/ sched and DAGScheduler
24/02/19 12:35:28 INFO StandaloneSchedulerBackend: appDeadline set to: 2147483647 for appName: TPC-H v3.0.0 Spark
24/02/19 12:35:28 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://130.207.125.81:7077...
24/02/19 12:35:28 INFO TransportClientFactory: Successfully created connection to /130.207.125.81:7077 after 35 ms (0 ms spent in bootstraps)
24/02/19 12:35:28 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240219123528-0008
24/02/19 12:35:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240219123528-0008/0 on worker-20240218173523-130.207.125.81-34605 (130.207.125.81:34605) with 24 core(s)
24/02/19 12:35:28 INFO StandaloneSchedulerBackend: Granted executor ID app-20240219123528-0008/0 on hostPort 130.207.125.81:34605 with 24 core(s), 1024.0 MiB RAM
24/02/19 12:35:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36207.
24/02/19 12:35:28 INFO NettyBlockTransferService: Server created on 130.207.125.81:36207
24/02/19 12:35:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/02/19 12:35:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 130.207.125.81, 36207, None)
24/02/19 12:35:28 INFO BlockManagerMasterEndpoint: Registering block manager 130.207.125.81:36207 with 434.4 MiB RAM, BlockManagerId(driver, 130.207.125.81, 36207, None)
24/02/19 12:35:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 130.207.125.81, 36207, None)
24/02/19 12:35:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 130.207.125.81, 36207, None)
24/02/19 12:35:28 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240219123528-0008/0 is now RUNNING
24/02/19 12:35:28 INFO RollingEventLogFilesWriter: Logging events to file:/tmp/spark-events/eventlog_v2_app-20240219123528-0008/events_1_app-20240219123528-0008.zstd
24/02/19 12:35:28 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@2149594a{/,null,STOPPED,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f3e9fbb{/jobs,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43471a7e{/jobs/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3cc053{/jobs/job,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7fbd3e75{/jobs/job/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7fdd43cd{/stages,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ce03e86{/stages/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7bee8621{/stages/stage,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1e12a5a6{/stages/stage/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@47a7c93e{/stages/pool,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@64ae105d{/stages/pool/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@69364b2d{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4792f119{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@ea00de{/storage,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@52f6900a{/storage/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2b80e5a9{/storage/rdd,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2a50b32d{/storage/rdd/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1ac6dd3d{/environment,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@794f11cd{/environment/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5d5c41e5{/executors,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@42e4e589{/executors/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@76f2dad9{/executors/threadDump,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@72b0a004{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2c1f8dbd{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@22825e1e{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@63fd4dda{/static,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5e9bbd9d{/,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d3e3085{/api,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@740dcae3{/jobs/job/kill,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@481b2f10{/stages/stage/kill,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5696c927{/metrics/json,null,AVAILABLE,@Spark}
24/02/19 12:35:28 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/02/19 12:35:29 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/02/19 12:35:29 INFO SharedState: Warehouse path is 'file:/home/dgarg39/tpch-spark/spark-warehouse'.
24/02/19 12:35:29 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d6a8386{/SQL,null,AVAILABLE,@Spark}
24/02/19 12:35:29 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@35cec305{/SQL/json,null,AVAILABLE,@Spark}
24/02/19 12:35:29 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@54c60202{/SQL/execution,null,AVAILABLE,@Spark}
24/02/19 12:35:29 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1e545821{/SQL/execution/json,null,AVAILABLE,@Spark}
24/02/19 12:35:29 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3b78c683{/static/sql,null,AVAILABLE,@Spark}
24/02/19 12:35:29 INFO InMemoryFileIndex: It took 31 ms to list leaf files for 1 paths.
24/02/19 12:35:30 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (130.207.125.81:56450) with ID 0,  ResourceProfileId 0
24/02/19 12:35:30 INFO BlockManagerMasterEndpoint: Registering block manager 130.207.125.81:38259 with 434.4 MiB RAM, BlockManagerId(0, 130.207.125.81, 38259, None)
24/02/19 12:35:31 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
24/02/19 12:35:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 12:35:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 12:35:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 12:35:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 12:35:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 12:35:31 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 12:35:32 INFO FileSourceStrategy: Pushed Filters: 
24/02/19 12:35:32 INFO FileSourceStrategy: Post-Scan Filters: 
24/02/19 12:35:32 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory
24/02/19 12:35:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
24/02/19 12:35:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
24/02/19 12:35:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
24/02/19 12:35:32 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/02/19 12:35:32 INFO CodeGenerator: Code generated in 281.296083 ms
24/02/19 12:35:33 INFO CodeGenerator: Code generated in 18.751378 ms
24/02/19 12:35:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.5 KiB, free 434.2 MiB)
24/02/19 12:35:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
24/02/19 12:35:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 130.207.125.81:36207 (size: 34.4 KiB, free: 434.4 MiB)
24/02/19 12:35:33 INFO SparkContext: Created broadcast 0 from save at TpchQuery.scala:35
24/02/19 12:35:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 31835732 bytes, open cost is considered as scanning 4194304 bytes.
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 0) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 1) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 2) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 3) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 4) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 5) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 6) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 7) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 8) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 9) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 10) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 11) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 12) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 13) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 14) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 15) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 16) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 17) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 18) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 19) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 20) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 21) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 22) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: DG: getPreferredLocs(rdd: MapPartitionsRDD[2] at save at TpchQuery.scala:35, partition: 23) on dagscheduler
24/02/19 12:35:33 INFO SparkContext: Starting job: save at TpchQuery.scala:35
24/02/19 12:35:33 INFO SparkContext: DG: RDD's recursive dependencies:
(1) MapPartitionsRDD[5] at save at TpchQuery.scala:35 []
 |  MapPartitionsRDD[4] at save at TpchQuery.scala:35 []
 |  CoalescedRDD[3] at save at TpchQuery.scala:35 []
 |  MapPartitionsRDD[2] at save at TpchQuery.scala:35 []
 |  MapPartitionsRDD[1] at save at TpchQuery.scala:35 []
 |  FileScanRDD[0] at save at TpchQuery.scala:35 []
24/02/19 12:35:33 INFO SparkContext: DG: runJob(rdd: MapPartitionsRDD[5] at save at TpchQuery.scala:35, partitions: Range 0 until 1) on dagscheduler
24/02/19 12:35:33 INFO DAGScheduler: DG: DAGScheduler runJob going to submitJob for rdd: MapPartitionsRDD[5] at save at TpchQuery.scala:35, func: org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3072/0x00007fc5ecea1648@373e09c7, partitions: Range 0 until 1
24/02/19 12:35:33 INFO DAGScheduler: DG: Inside submitJob for jobId: 0
24/02/19 12:35:33 INFO DAGScheduler: DG: submitJob partition is not empty
24/02/19 12:35:33 INFO DAGScheduler: DG: submiting jobID 0 with partitions: 1
24/02/19 12:35:33 INFO DAGScheduler: DG: getOrCreateParentStages(shuffleDeps: HashSet(), firstJobId: 0)
24/02/19 12:35:33 INFO DAGScheduler: DG: createResultStage(id: 0, rdd: MapPartitionsRDD[5] at save at TpchQuery.scala:35, func: org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$3072/0x00007fc5ecea1648@373e09c7, partitions: [I@1562bf36 , parents: List(), jobId: 0, resourceProfile.id: 0)
24/02/19 12:35:33 INFO DAGScheduler: DG: stageIdToStage in createResultStage:HashMap(0 -> ResultStage 0)
24/02/19 12:35:33 INFO DAGScheduler: DG: invoking updateJobIdStageIdMaps(jobId: 0, stage: ResultStage 0) from createResultStage
24/02/19 12:35:33 INFO DAGScheduler: Invoking updateJobIdStageIdMaps for jobId 0 and stage ResultStage 0
24/02/19 12:35:33 INFO DAGScheduler: DG: after updating updateJobIdStageIdMaps, jobIdToStageIds: HashMap(0 -> HashSet(0))
24/02/19 12:35:33 INFO DAGScheduler: Got job 0 (save at TpchQuery.scala:35) with 1 output partitions
24/02/19 12:35:33 INFO DAGScheduler: Final stage: ResultStage 0 (save at TpchQuery.scala:35)
24/02/19 12:35:33 INFO DAGScheduler: Parents of final stage: List()
24/02/19 12:35:33 INFO DAGScheduler: Missing parents: List()
24/02/19 12:35:33 INFO DAGScheduler: [ERDOS] Retrieving the dependency graph starting at save at TpchQuery.scala:35
24/02/19 12:35:33 INFO DAGScheduler: [ERDOS] Visiting stage ResultStage 0 (name=save at TpchQuery.scala:35, id=0) and finding its parents.
24/02/19 12:35:33 INFO DAGScheduler: Dependency Graph: HashMap(0 -> List())
24/02/19 12:35:33 INFO DAGScheduler: DG: Invoking submitStage with finalStage(ResultStage 0) in handleJobSubmitted
24/02/19 12:35:33 INFO DAGScheduler: DG: activeJobForStage( ResultStage 0 ): is jobId: Some(0)
24/02/19 12:35:33 INFO DAGScheduler: submitStage(ResultStage 0 (name=save at TpchQuery.scala:35;jobs=0))
24/02/19 12:35:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[5] at save at TpchQuery.scala:35), which has no missing parents
24/02/19 12:35:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 319.8 KiB, free 433.9 MiB)
24/02/19 12:35:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 108.2 KiB, free 433.8 MiB)
24/02/19 12:35:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 130.207.125.81:36207 (size: 108.2 KiB, free: 434.3 MiB)
24/02/19 12:35:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1668
24/02/19 12:35:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[5] at save at TpchQuery.scala:35) (first 15 tasks are for partitions Vector(0))
24/02/19 12:35:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/02/19 12:35:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (130.207.125.81, executor 0, partition 0, PROCESS_LOCAL, 10744 bytes) 
24/02/19 12:35:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 130.207.125.81:38259 (size: 108.2 KiB, free: 434.3 MiB)
24/02/19 12:35:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 130.207.125.81:38259 (size: 34.4 KiB, free: 434.3 MiB)
24/02/19 12:35:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 9691 ms on 130.207.125.81 (executor 0) (1/1)
24/02/19 12:35:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/02/19 12:35:43 INFO DAGScheduler: ResultStage 0 (save at TpchQuery.scala:35) finished in 9.773 s
24/02/19 12:35:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/02/19 12:35:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/02/19 12:35:43 INFO DAGScheduler: Job 0 finished: save at TpchQuery.scala:35, took 9.818468 s
24/02/19 12:35:43 INFO FileFormatWriter: Start to commit write Job f62cac99-9eeb-4191-b7bd-0141e1d1e2f5.
24/02/19 12:35:43 INFO FileFormatWriter: Write Job f62cac99-9eeb-4191-b7bd-0141e1d1e2f5 committed. Elapsed time: 17 ms.
24/02/19 12:35:43 INFO FileFormatWriter: Finished processing stats for write job f62cac99-9eeb-4191-b7bd-0141e1d1e2f5.
24/02/19 12:35:43 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/02/19 12:35:43 INFO AbstractConnector: Stopped Spark@6a4e09fe{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
24/02/19 12:35:43 INFO SparkUI: Stopped Spark web UI at http://130.207.125.81:4040
24/02/19 12:35:43 INFO StandaloneSchedulerBackend: Shutting down all executors
24/02/19 12:35:43 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
24/02/19 12:35:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/02/19 12:35:43 INFO MemoryStore: MemoryStore cleared
24/02/19 12:35:43 INFO BlockManager: BlockManager stopped
24/02/19 12:35:43 INFO BlockManagerMaster: BlockManagerMaster stopped
24/02/19 12:35:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/02/19 12:35:43 INFO SparkContext: Successfully stopped SparkContext
Execution times written in /home/dgarg39/tpch-spark/tpch-times.txt.
Execution complete.
24/02/19 12:35:43 INFO ShutdownHookManager: Shutdown hook called
24/02/19 12:35:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-fc93946f-059f-423c-bc03-91243f5ea655
24/02/19 12:35:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-2c8bd3a0-c712-4666-96c6-4793bf094f6f
