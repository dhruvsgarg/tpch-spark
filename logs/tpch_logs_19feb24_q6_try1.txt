24/02/19 18:35:04 WARN Utils: Your hostname, rivertam resolves to a loopback address: 127.0.1.1; using 130.207.125.81 instead (on interface ens6f0)
24/02/19 18:35:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
24/02/19 18:35:04 INFO SparkContext: Running Spark version 4.0.0-SNAPSHOT
24/02/19 18:35:04 INFO SparkContext: OS info Linux, 5.4.0-152-generic, amd64
24/02/19 18:35:04 INFO SparkContext: Java version 17.0.9-internal
24/02/19 18:35:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/02/19 18:35:04 INFO SparkContext: Application specified no deadline, will be set to Int.MaxValue (string)
24/02/19 18:35:04 INFO SparkContext: Application specified no tpch_qtype, will be set to NONE
24/02/19 18:35:04 INFO ResourceUtils: ==============================================================
24/02/19 18:35:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/02/19 18:35:04 INFO ResourceUtils: ==============================================================
24/02/19 18:35:04 INFO SparkContext: Submitted application: TPC-H v3.0.0 Spark
24/02/19 18:35:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/02/19 18:35:04 INFO ResourceProfile: Limiting resource is cpu
24/02/19 18:35:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/02/19 18:35:04 INFO SecurityManager: Changing view acls to: dgarg39
24/02/19 18:35:04 INFO SecurityManager: Changing modify acls to: dgarg39
24/02/19 18:35:04 INFO SecurityManager: Changing view acls groups to: 
24/02/19 18:35:04 INFO SecurityManager: Changing modify acls groups to: 
24/02/19 18:35:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dgarg39; groups with view permissions: EMPTY; users with modify permissions: dgarg39; groups with modify permissions: EMPTY; RPC SSL disabled
24/02/19 18:35:05 INFO Utils: Successfully started service 'sparkDriver' on port 44043.
24/02/19 18:35:05 INFO SparkEnv: Registering MapOutputTracker
24/02/19 18:35:05 INFO SecurityManager: Changing view acls to: dgarg39
24/02/19 18:35:05 INFO SecurityManager: Changing modify acls to: dgarg39
24/02/19 18:35:05 INFO SecurityManager: Changing view acls groups to: 
24/02/19 18:35:05 INFO SecurityManager: Changing modify acls groups to: 
24/02/19 18:35:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dgarg39; groups with view permissions: EMPTY; users with modify permissions: dgarg39; groups with modify permissions: EMPTY; RPC SSL disabled
24/02/19 18:35:05 INFO SparkEnv: Registering BlockManagerMaster
24/02/19 18:35:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/02/19 18:35:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/02/19 18:35:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/02/19 18:35:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b4c3eeb8-07b7-4c6d-838b-e164058827e6
24/02/19 18:35:05 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/02/19 18:35:05 INFO SparkEnv: Registering OutputCommitCoordinator
24/02/19 18:35:05 INFO log: Logging initialized @1686ms to org.eclipse.jetty.util.log.Slf4jLog
24/02/19 18:35:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/02/19 18:35:05 INFO Server: jetty-9.4.53.v20231009; built: 2023-10-09T12:29:09.265Z; git: 27bde00a0b95a1d5bbee0eae7984f891d2d0f8c9; jvm 17.0.9-internal+0-adhoc..src
24/02/19 18:35:05 INFO Server: Started @1765ms
24/02/19 18:35:05 INFO AbstractConnector: Started ServerConnector@6a4e09fe{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
24/02/19 18:35:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/02/19 18:35:05 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2149594a{/,null,AVAILABLE,@Spark}
24/02/19 18:35:05 INFO SparkContext: Added JAR file:/home/dgarg39/tpch-spark/target/scala-2.13/spark-tpc-h-queries_2.13-1.0.jar at spark://130.207.125.81:44043/jars/spark-tpc-h-queries_2.13-1.0.jar with timestamp 1708385704656
24/02/19 18:35:05 INFO SparkContext: DG: init TaskScheduler w/ sched and DAGScheduler
24/02/19 18:35:05 INFO StandaloneSchedulerBackend: appDeadline set to: 2147483647 for appName: TPC-H v3.0.0 Spark
24/02/19 18:35:05 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://130.207.125.81:7077...
24/02/19 18:35:05 INFO TransportClientFactory: Successfully created connection to /130.207.125.81:7077 after 35 ms (0 ms spent in bootstraps)
24/02/19 18:35:05 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240219183505-0035
24/02/19 18:35:05 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240219183505-0035/0 on worker-20240218173523-130.207.125.81-34605 (130.207.125.81:34605) with 24 core(s)
24/02/19 18:35:05 INFO StandaloneSchedulerBackend: Granted executor ID app-20240219183505-0035/0 on hostPort 130.207.125.81:34605 with 24 core(s), 1024.0 MiB RAM
24/02/19 18:35:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46537.
24/02/19 18:35:05 INFO NettyBlockTransferService: Server created on 130.207.125.81:46537
24/02/19 18:35:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/02/19 18:35:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 130.207.125.81, 46537, None)
24/02/19 18:35:05 INFO BlockManagerMasterEndpoint: Registering block manager 130.207.125.81:46537 with 434.4 MiB RAM, BlockManagerId(driver, 130.207.125.81, 46537, None)
24/02/19 18:35:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 130.207.125.81, 46537, None)
24/02/19 18:35:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 130.207.125.81, 46537, None)
24/02/19 18:35:05 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240219183505-0035/0 is now RUNNING
24/02/19 18:35:05 INFO RollingEventLogFilesWriter: Logging events to file:/tmp/spark-events/eventlog_v2_app-20240219183505-0035/events_1_app-20240219183505-0035.zstd
24/02/19 18:35:06 INFO ContextHandler: Stopped o.e.j.s.ServletContextHandler@2149594a{/,null,STOPPED,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@365afe87{/jobs,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@21e45a6f{/jobs/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@388c519{/jobs/job,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@198536f6{/jobs/job/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3a38f122{/stages,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1c628f6a{/stages/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4e0cc334{/stages/stage,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4877919f{/stages/stage/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6f9ab79d{/stages/pool,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@10a18e3e{/stages/pool/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@53aa2fc9{/stages/taskThreadDump,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5bde57ab{/stages/taskThreadDump/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@23ca36d{/storage,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7bbcf6f0{/storage/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4cc01c7f{/storage/rdd,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@36c7cbe1{/storage/rdd/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@562919fe{/environment,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@715f45c6{/environment/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@43245559{/executors,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@499ee966{/executors/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@171dc7c3{/executors/threadDump,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4cdba2ed{/executors/threadDump/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@55c57422{/executors/heapHistogram,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@335f5c69{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d6751e3{/static,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4b87074a{/,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2ba42204{/api,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4e140497{/jobs/job/kill,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2b6c7012{/stages/stage/kill,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7459a21e{/metrics/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/02/19 18:35:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/02/19 18:35:06 INFO SharedState: Warehouse path is 'file:/home/dgarg39/tpch-spark/spark-warehouse'.
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6274f21c{/SQL,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@237add{/SQL/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7889b4b9{/SQL/execution,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@12952aff{/SQL/execution/json,null,AVAILABLE,@Spark}
24/02/19 18:35:06 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7f5614f9{/static/sql,null,AVAILABLE,@Spark}
24/02/19 18:35:07 INFO InMemoryFileIndex: It took 46 ms to list leaf files for 1 paths.
24/02/19 18:35:07 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (130.207.125.81:40348) with ID 0,  ResourceProfileId 0
24/02/19 18:35:07 INFO BlockManagerMasterEndpoint: Registering block manager 130.207.125.81:37959 with 434.4 MiB RAM, BlockManagerId(0, 130.207.125.81, 37959, None)
24/02/19 18:35:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
24/02/19 18:35:08 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
24/02/19 18:35:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 18:35:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 18:35:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 18:35:08 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 18:35:09 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
24/02/19 18:35:09 INFO FileSourceStrategy: Pushed Filters: 
24/02/19 18:35:09 INFO FileSourceStrategy: Post-Scan Filters: 
24/02/19 18:35:09 INFO PathOutputCommitterFactory: No output committer factory defined, defaulting to FileOutputCommitterFactory
24/02/19 18:35:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
24/02/19 18:35:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
24/02/19 18:35:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
24/02/19 18:35:09 INFO CodeGenerator: Code generated in 156.1519 ms
24/02/19 18:35:09 INFO CodeGenerator: Code generated in 29.779046 ms
24/02/19 18:35:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.5 KiB, free 434.2 MiB)
24/02/19 18:35:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
24/02/19 18:35:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 130.207.125.81:46537 (size: 34.3 KiB, free: 434.4 MiB)
24/02/19 18:35:10 INFO SparkContext: Created broadcast 0 from save at TpchQuery.scala:35
24/02/19 18:35:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 31835732 bytes, open cost is considered as scanning 4194304 bytes.
24/02/19 18:35:10 INFO SparkContext: Starting job: save at TpchQuery.scala:35
24/02/19 18:35:10 INFO SparkContext: DG: RDD's recursive dependencies:
(1) MapPartitionsRDD[6] at save at TpchQuery.scala:35 []
 |  MapPartitionsRDD[5] at save at TpchQuery.scala:35 []
 |  ShuffledRowRDD[4] at save at TpchQuery.scala:35 []
 +-(24) MapPartitionsRDD[3] at save at TpchQuery.scala:35 []
    |   MapPartitionsRDD[2] at save at TpchQuery.scala:35 []
    |   MapPartitionsRDD[1] at save at TpchQuery.scala:35 []
    |   FileScanRDD[0] at save at TpchQuery.scala:35 []
24/02/19 18:35:10 INFO SparkContext: DG: runJob(rdd: MapPartitionsRDD[6] at save at TpchQuery.scala:35, partitions: Range 0 until 1) on dagscheduler
24/02/19 18:35:10 INFO DAGScheduler: DG: DAGScheduler runJob going to submitJob for rdd: MapPartitionsRDD[6] at save at TpchQuery.scala:35, func: org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$2969/0x00007fce94e4e2d8@3f16a823, partitions: Range 0 until 1
24/02/19 18:35:10 INFO DAGScheduler: DG: Inside submitJob for jobId: 0
24/02/19 18:35:10 INFO DAGScheduler: DG: submitJob partition is not empty
24/02/19 18:35:10 INFO DAGScheduler: DG: submiting jobID 0 with partitions: 1
24/02/19 18:35:10 INFO DAGScheduler: DG: getOrCreateParentStages(shuffleDeps: HashSet(org.apache.spark.ShuffleDependency@1e26e6dd), firstJobId: 0)
24/02/19 18:35:10 INFO DAGScheduler: DG: in getMissingAncestorShuffleDependencies(rdd: MapPartitionsRDD[3] at save at TpchQuery.scala:35), waitingForVisit: ListBuffer(MapPartitionsRDD[3] at save at TpchQuery.scala:35)
24/02/19 18:35:10 INFO DAGScheduler: DG:  found ancestors: ListBuffer()
24/02/19 18:35:10 INFO DAGScheduler: DG: getMissingAncestorShuffleDependencies() has more info on visited, waitingForVisit. Re-check this for more info
24/02/19 18:35:10 INFO DAGScheduler: DG: createShuffleMapStage(shuffleDep: org.apache.spark.ShuffleDependency@1e26e6dd, firstJobId: 0)
24/02/19 18:35:10 INFO DAGScheduler: DG: getOrCreateParentStages(shuffleDeps: HashSet(), firstJobId: 0)
24/02/19 18:35:10 INFO DAGScheduler: DG: in createShuffleMapStage: id: 0, rdd: MapPartitionsRDD[3] at save at TpchQuery.scala:35, numTasks: 24 , parents: List(), jobId: 0, resourceProfile.id: 0
24/02/19 18:35:10 INFO DAGScheduler: DG: stageIdToStage in createShuffleMapStage:HashMap(0 -> ShuffleMapStage 0)
24/02/19 18:35:10 INFO DAGScheduler: DG: shuffleIdToMapStage in createShuffleMapStage:HashMap(0 -> ShuffleMapStage 0)
24/02/19 18:35:10 INFO DAGScheduler: DG: invoking updateJobIdStageIdMaps(jobId: 0, stage: ShuffleMapStage 0) from createShuffleMapStage:HashMap(0 -> ShuffleMapStage 0)
24/02/19 18:35:10 INFO DAGScheduler: Invoking updateJobIdStageIdMaps for jobId 0 and stage ShuffleMapStage 0
24/02/19 18:35:10 INFO DAGScheduler: DG: after updating updateJobIdStageIdMaps, jobIdToStageIds: HashMap(0 -> HashSet(0))
24/02/19 18:35:10 INFO DAGScheduler: Registering RDD 3 (save at TpchQuery.scala:35) as input to shuffle 0
24/02/19 18:35:10 INFO DAGScheduler: DG: registered RDDs with mapOutputTracker. Will return ShuffleMapStage now
24/02/19 18:35:10 INFO DAGScheduler: DG: createResultStage(id: 1, rdd: MapPartitionsRDD[6] at save at TpchQuery.scala:35, func: org.apache.spark.sql.execution.datasources.FileFormatWriter$$$Lambda$2969/0x00007fce94e4e2d8@3f16a823, partitions: [I@73177776 , parents: List(ShuffleMapStage 0), jobId: 0, resourceProfile.id: 0)
24/02/19 18:35:10 INFO DAGScheduler: DG: stageIdToStage in createResultStage:HashMap(0 -> ShuffleMapStage 0, 1 -> ResultStage 1)
24/02/19 18:35:10 INFO DAGScheduler: DG: invoking updateJobIdStageIdMaps(jobId: 0, stage: ResultStage 1) from createResultStage
24/02/19 18:35:10 INFO DAGScheduler: Invoking updateJobIdStageIdMaps for jobId 0 and stage ResultStage 1
24/02/19 18:35:10 INFO DAGScheduler: DG: after updating updateJobIdStageIdMaps, jobIdToStageIds: HashMap(0 -> HashSet(0, 1))
24/02/19 18:35:10 INFO DAGScheduler: Got job 0 (save at TpchQuery.scala:35) with 1 output partitions
24/02/19 18:35:10 INFO DAGScheduler: Final stage: ResultStage 1 (save at TpchQuery.scala:35)
24/02/19 18:35:10 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
24/02/19 18:35:10 INFO DAGScheduler: DG: getOrCreateShuffleMapStage(shuffleDep: org.apache.spark.ShuffleDependency@1e26e6dd, firstJobId: 0) found stage: ShuffleMapStage 0
24/02/19 18:35:10 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
24/02/19 18:35:10 INFO DAGScheduler: [ERDOS] Retrieving the dependency graph starting at save at TpchQuery.scala:35
24/02/19 18:35:10 INFO DAGScheduler: [ERDOS] Visiting stage ResultStage 1 (name=save at TpchQuery.scala:35, id=1) and finding its parents.
24/02/19 18:35:10 INFO DAGScheduler: DG: getOrCreateShuffleMapStage(shuffleDep: org.apache.spark.ShuffleDependency@1e26e6dd, firstJobId: 0) found stage: ShuffleMapStage 0
24/02/19 18:35:10 INFO DAGScheduler: [ERDOS] Stage ShuffleMapStage 0 (name=save at TpchQuery.scala:35, id=0) is a parent of ResultStage 1 (name=save at TpchQuery.scala:35, id=1)
24/02/19 18:35:10 INFO DAGScheduler: [ERDOS] Visiting stage ShuffleMapStage 0 (name=save at TpchQuery.scala:35, id=0) and finding its parents.
24/02/19 18:35:10 INFO DAGScheduler: Dependency Graph: HashMap(0 -> List(1), 1 -> List())
24/02/19 18:35:10 INFO DAGScheduler: DG: Invoking submitStage with finalStage(ResultStage 1) in handleJobSubmitted
24/02/19 18:35:10 INFO DAGScheduler: DG: activeJobForStage( ResultStage 1 ): is jobId: Some(0)
24/02/19 18:35:10 INFO DAGScheduler: submitStage(ResultStage 1 (name=save at TpchQuery.scala:35;jobs=0))
24/02/19 18:35:10 INFO DAGScheduler: DG: getOrCreateShuffleMapStage(shuffleDep: org.apache.spark.ShuffleDependency@1e26e6dd, firstJobId: 0) found stage: ShuffleMapStage 0
24/02/19 18:35:10 INFO DAGScheduler: DG: submitStage(parent): since stage: ResultStage 1 is missing parent: ShuffleMapStage 0
24/02/19 18:35:10 INFO DAGScheduler: DG: activeJobForStage( ShuffleMapStage 0 ): is jobId: Some(0)
24/02/19 18:35:10 INFO DAGScheduler: submitStage(ShuffleMapStage 0 (name=save at TpchQuery.scala:35;jobs=0))
24/02/19 18:35:10 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at save at TpchQuery.scala:35), which has no missing parents
24/02/19 18:35:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 29.4 KiB, free 434.1 MiB)
24/02/19 18:35:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.1 MiB)
24/02/19 18:35:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 130.207.125.81:46537 (size: 10.2 KiB, free: 434.4 MiB)
24/02/19 18:35:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1668
24/02/19 18:35:10 INFO DAGScheduler: Submitting 24 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at save at TpchQuery.scala:35) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
24/02/19 18:35:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 24 tasks resource profile 0
24/02/19 18:35:10 INFO DAGScheduler: DG: adding stage: ResultStage 1 to waitingStages: HashSet(ResultStage 1)
24/02/19 18:35:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (130.207.125.81, executor 0, partition 0, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (130.207.125.81, executor 0, partition 1, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (130.207.125.81, executor 0, partition 2, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (130.207.125.81, executor 0, partition 3, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (130.207.125.81, executor 0, partition 4, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (130.207.125.81, executor 0, partition 5, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (130.207.125.81, executor 0, partition 6, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (130.207.125.81, executor 0, partition 7, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (130.207.125.81, executor 0, partition 8, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (130.207.125.81, executor 0, partition 9, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (130.207.125.81, executor 0, partition 10, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (130.207.125.81, executor 0, partition 11, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (130.207.125.81, executor 0, partition 12, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (130.207.125.81, executor 0, partition 13, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (130.207.125.81, executor 0, partition 14, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (130.207.125.81, executor 0, partition 15, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (130.207.125.81, executor 0, partition 16, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (130.207.125.81, executor 0, partition 17, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (130.207.125.81, executor 0, partition 18, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (130.207.125.81, executor 0, partition 19, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 20.0 in stage 0.0 (TID 20) (130.207.125.81, executor 0, partition 20, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 21.0 in stage 0.0 (TID 21) (130.207.125.81, executor 0, partition 21, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 22.0 in stage 0.0 (TID 22) (130.207.125.81, executor 0, partition 22, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO TaskSetManager: Starting task 23.0 in stage 0.0 (TID 23) (130.207.125.81, executor 0, partition 23, PROCESS_LOCAL, 8494 bytes) 
24/02/19 18:35:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 130.207.125.81:37959 (size: 10.2 KiB, free: 434.4 MiB)
24/02/19 18:35:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 130.207.125.81:37959 (size: 34.3 KiB, free: 434.4 MiB)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 2462 ms on 130.207.125.81 (executor 0) (1/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 21.0 in stage 0.0 (TID 21) in 2467 ms on 130.207.125.81 (executor 0) (2/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2476 ms on 130.207.125.81 (executor 0) (3/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 2471 ms on 130.207.125.81 (executor 0) (4/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2498 ms on 130.207.125.81 (executor 0) (5/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 2476 ms on 130.207.125.81 (executor 0) (6/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 2474 ms on 130.207.125.81 (executor 0) (7/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 2474 ms on 130.207.125.81 (executor 0) (8/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 2479 ms on 130.207.125.81 (executor 0) (9/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 23.0 in stage 0.0 (TID 23) in 2473 ms on 130.207.125.81 (executor 0) (10/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 2474 ms on 130.207.125.81 (executor 0) (11/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 20.0 in stage 0.0 (TID 20) in 2475 ms on 130.207.125.81 (executor 0) (12/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 2480 ms on 130.207.125.81 (executor 0) (13/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2482 ms on 130.207.125.81 (executor 0) (14/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 22.0 in stage 0.0 (TID 22) in 2483 ms on 130.207.125.81 (executor 0) (15/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 2493 ms on 130.207.125.81 (executor 0) (16/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 2493 ms on 130.207.125.81 (executor 0) (17/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2496 ms on 130.207.125.81 (executor 0) (18/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 2494 ms on 130.207.125.81 (executor 0) (19/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 2498 ms on 130.207.125.81 (executor 0) (20/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 2501 ms on 130.207.125.81 (executor 0) (21/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 2507 ms on 130.207.125.81 (executor 0) (22/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 2514 ms on 130.207.125.81 (executor 0) (23/24)
24/02/19 18:35:12 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 2539 ms on 130.207.125.81 (executor 0) (24/24)
24/02/19 18:35:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/02/19 18:35:12 INFO DAGScheduler: ShuffleMapStage 0 (save at TpchQuery.scala:35) finished in 2.640 s
24/02/19 18:35:12 INFO DAGScheduler: looking for newly runnable stages
24/02/19 18:35:12 INFO DAGScheduler: running: HashSet()
24/02/19 18:35:12 INFO DAGScheduler: waiting: HashSet(ResultStage 1)
24/02/19 18:35:12 INFO DAGScheduler: failed: HashSet()
24/02/19 18:35:12 INFO DAGScheduler: Checking if any dependencies of ShuffleMapStage 0 are now runnable
24/02/19 18:35:12 INFO DAGScheduler: running: HashSet()
24/02/19 18:35:12 INFO DAGScheduler: waiting: HashSet(ResultStage 1)
24/02/19 18:35:12 INFO DAGScheduler: failed: HashSet()
24/02/19 18:35:12 INFO DAGScheduler: DG: Invoking submitStage(ResultStage 1) in submitWaitingChildStages
24/02/19 18:35:12 INFO DAGScheduler: DG: activeJobForStage( ResultStage 1 ): is jobId: Some(0)
24/02/19 18:35:12 INFO DAGScheduler: submitStage(ResultStage 1 (name=save at TpchQuery.scala:35;jobs=0))
24/02/19 18:35:12 INFO DAGScheduler: DG: getOrCreateShuffleMapStage(shuffleDep: org.apache.spark.ShuffleDependency@1e26e6dd, firstJobId: 0) found stage: ShuffleMapStage 0
24/02/19 18:35:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at save at TpchQuery.scala:35), which has no missing parents
24/02/19 18:35:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 216.2 KiB, free 433.9 MiB)
24/02/19 18:35:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 78.1 KiB, free 433.8 MiB)
24/02/19 18:35:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 130.207.125.81:46537 (size: 78.1 KiB, free: 434.3 MiB)
24/02/19 18:35:12 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1668
24/02/19 18:35:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at save at TpchQuery.scala:35) (first 15 tasks are for partitions Vector(0))
24/02/19 18:35:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/02/19 18:35:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 24) (130.207.125.81, executor 0, partition 0, NODE_LOCAL, 7892 bytes) 
24/02/19 18:35:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 130.207.125.81:37959 (size: 78.1 KiB, free: 434.3 MiB)
24/02/19 18:35:13 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 130.207.125.81:40348
24/02/19 18:35:13 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 24) in 262 ms on 130.207.125.81 (executor 0) (1/1)
24/02/19 18:35:13 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/02/19 18:35:13 INFO DAGScheduler: ResultStage 1 (save at TpchQuery.scala:35) finished in 0.306 s
24/02/19 18:35:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/02/19 18:35:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/02/19 18:35:13 INFO DAGScheduler: Job 0 finished: save at TpchQuery.scala:35, took 3.036820 s
24/02/19 18:35:13 INFO FileFormatWriter: Start to commit write Job 0b27b3f5-00b6-4d04-b7e3-63d100521c5d.
24/02/19 18:35:13 INFO FileFormatWriter: Write Job 0b27b3f5-00b6-4d04-b7e3-63d100521c5d committed. Elapsed time: 17 ms.
24/02/19 18:35:13 INFO FileFormatWriter: Finished processing stats for write job 0b27b3f5-00b6-4d04-b7e3-63d100521c5d.
24/02/19 18:35:13 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/02/19 18:35:13 INFO AbstractConnector: Stopped Spark@6a4e09fe{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
24/02/19 18:35:13 INFO SparkUI: Stopped Spark web UI at http://130.207.125.81:4040
24/02/19 18:35:13 INFO StandaloneSchedulerBackend: Shutting down all executors
24/02/19 18:35:13 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
24/02/19 18:35:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/02/19 18:35:13 INFO MemoryStore: MemoryStore cleared
24/02/19 18:35:13 INFO BlockManager: BlockManager stopped
24/02/19 18:35:13 INFO BlockManagerMaster: BlockManagerMaster stopped
24/02/19 18:35:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/02/19 18:35:13 INFO SparkContext: Successfully stopped SparkContext
Execution times written in /home/dgarg39/tpch-spark/tpch-times.txt.
Execution complete.
24/02/19 18:35:13 INFO ShutdownHookManager: Shutdown hook called
24/02/19 18:35:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-17f1f5c2-13e4-41dd-9762-162283e2885f
24/02/19 18:35:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-030f2bba-342c-470a-b1f7-1bf01ae32c3e
